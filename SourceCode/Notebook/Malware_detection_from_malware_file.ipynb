{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlmgGOY5qBoi"
      },
      "source": [
        "# Prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtvr3aJ8Wn72",
        "outputId": "9a30c91a-70c6-4210-a6b9-84acdb3700f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFb6OuHh7sq",
        "outputId": "f471cf32-0a46-4f1b-de92-e8b93d5ddbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting pefile\n",
            "  Downloading pefile-2023.2.7-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m412.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chainer\n",
            "  Downloading chainer-7.8.1.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from chainer) (67.7.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from chainer) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from chainer) (3.14.0)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from chainer) (3.20.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from chainer) (1.16.0)\n",
            "Building wheels for collected packages: chainer\n",
            "  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chainer: filename=chainer-7.8.1-py3-none-any.whl size=967716 sha256=9545f54716dcc70805414f8dc168388ab1d8a318a66e91b18b05e4264d1d4f4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/95/6a/16014db6f761c4e742755b64aac60dbe142da1df6c5919f790\n",
            "Successfully built chainer\n",
            "Installing collected packages: pefile, chainer\n",
            "Successfully installed chainer-7.8.1 pefile-2023.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost pefile chainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSiBXMAMPFH_",
        "outputId": "4858ecd9-1f0d-48f0-86cf-25f3177fda20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Thêm các thư viện cần thiết và đường dẫn tập dữ liệu để chia train và test\n",
        "import os\n",
        "import re\n",
        "import pefile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "Benign_path = [\"/content/drive/MyDrive/Dataset/Malware Analysis/Benign PE Samples\",\n",
        "               \"/content/drive/MyDrive/Dataset/Malware Analysis/Benign PE Samples Amber\",\n",
        "               \"/content/drive/MyDrive/Dataset/Malware Analysis/Benign PE Samples UPX\"]\n",
        "Malicious_path = [\"/content/drive/MyDrive/Dataset/Malware Analysis/Malicious PE Samples\",\n",
        "                  \"/content/drive/MyDrive/Dataset/Malware Analysis/Malware_sample\"]\n",
        "\n",
        "benign_files = []\n",
        "for path in Benign_path:\n",
        "    benign_files.extend(glob(path + '/*'))  # Use extend to add elements of a list to another list\n",
        "benign_files = [file for file in benign_files if os.path.getsize(file) > 0]\n",
        "\n",
        "malicious_files = []\n",
        "for path in Malicious_path:\n",
        "    malicious_files.extend(glob(path + '/*')) # Use extend to add elements of a list to another list\n",
        "malicious_files = [file for file in malicious_files if os.path.getsize(file) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff9cuS5EcUWB"
      },
      "outputs": [],
      "source": [
        "LANGUAGE_MODEL = \"Doc2Vec\"\n",
        "# LANGUAGE_MODEL = \"LSI\"\n",
        "\n",
        "# CLASSIFIER_MODEL = \"SVM\"\n",
        "# CLASSIFIER_MODEL = \"RF\"\n",
        "# CLASSIFIER_MODEL = \"XGB\"\n",
        "# CLASSIFIER_MODEL = \"MLP\"\n",
        "# CLASSIFIER_MODEL = \"CNN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0YiGpKUqsOo"
      },
      "source": [
        "# Step 1: Extract printable strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBrzPoU83dtg"
      },
      "outputs": [],
      "source": [
        "MAX_FILESIZE = 16*1024*1024\n",
        "MAX_STRINGCNT = 2048\n",
        "MAX_STRINGLEN = 1024\n",
        "\n",
        "def Strings(file_path):\n",
        "    data = open(file_path, \"rb\").read(MAX_FILESIZE)\n",
        "    strings = []\n",
        "    for s in re.findall(b\"[\\x1f-\\x7e]{6,}\", data):\n",
        "        strings.append(s.decode(\"utf-8\"))\n",
        "    for s in re.findall(b\"(?:[\\x1f-\\x7e][\\x00]){6,}\", data):\n",
        "        strings.append(s.decode(\"utf-16le\"))\n",
        "\n",
        "    # Now limit the amount & length of the strings.\n",
        "    strings = strings[:MAX_STRINGCNT]\n",
        "    for idx, s in enumerate(strings):\n",
        "        strings[idx] = s[:MAX_STRINGLEN]\n",
        "\n",
        "    return strings\n",
        "\n",
        "\n",
        "def String2word(string_list):\n",
        "    # Join the list of strings into a single string\n",
        "    full_string = ' '.join(string_list)\n",
        "    # Split the full string into words\n",
        "    return full_string.lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3dyRrLx7SdU"
      },
      "outputs": [],
      "source": [
        "# Extract printable strings\n",
        "malware_words = []\n",
        "benign_words = []\n",
        "\n",
        "# Extract printable strings from malware samples\n",
        "for file_path in malicious_files:\n",
        "    strings = Strings(file_path)\n",
        "    words = String2word(strings)\n",
        "    malware_words.append(words)\n",
        "\n",
        "# Extract printable strings from benign samples\n",
        "for file_path in benign_files:\n",
        "    strings = Strings(file_path)\n",
        "    words = String2word(strings)\n",
        "    benign_words.append(words)\n",
        "\n",
        "del strings, words, file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7RA5l28rKgA"
      },
      "source": [
        "#  Step 2: Select the words with high frequency and create a corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOixXxVpAamu"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def Select_frequent_words(samples, num_unique_words):\n",
        "    word_counts = Counter()\n",
        "    for words in samples:\n",
        "        word_counts.update(words)\n",
        "\n",
        "    # Select top num_unique_words most common words\n",
        "    frequent_words = [word for word, _ in word_counts.most_common(num_unique_words)]\n",
        "    return frequent_words\n",
        "\n",
        "if LANGUAGE_MODEL == \"Doc2Vec\":\n",
        "    benign_frequent_words = Select_frequent_words(benign_words, num_unique_words=500)\n",
        "    malware_frequent_words = Select_frequent_words(malware_words, num_unique_words=500)\n",
        "else:\n",
        "    benign_frequent_words = Select_frequent_words(benign_words, num_unique_words=9000)\n",
        "    malware_frequent_words = Select_frequent_words(malware_words, num_unique_words=9000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xesvI32suvT"
      },
      "outputs": [],
      "source": [
        "def Reduce_words(words, frequent_words):\n",
        "    reduced_words = []\n",
        "    for file_words in words:\n",
        "        reduced_file_words = [word for word in file_words if word in frequent_words]\n",
        "        reduced_words.append(reduced_file_words)\n",
        "    return reduced_words\n",
        "\n",
        "reduced_benign_words = Reduce_words(benign_words, benign_frequent_words)\n",
        "reduced_malware_words = Reduce_words(malware_words, malware_frequent_words)\n",
        "corpus = reduced_benign_words + reduced_malware_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoAUvhLkrmfr"
      },
      "source": [
        "# Step 3: Construct language models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs9oU9qYhnDC"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Doc2Vec, LsiModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "def tagged_document(list_of_list_of_words):\n",
        "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
        "        yield TaggedDocument(list_of_words, [i])\n",
        "\n",
        "def Construct_language_model(corpus, model_type):\n",
        "    if model_type == \"Doc2Vec\":\n",
        "        data_for_training = list(tagged_document(corpus))\n",
        "        model = Doc2Vec(vector_size=400, alpha=0.075, min_count=2, window=1, epochs=20)\n",
        "        model.build_vocab(data_for_training)\n",
        "        model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "        return model\n",
        "    else:\n",
        "        # Construct a dictionary from corpus\n",
        "        dictionary = Dictionary(corpus)\n",
        "        # Convert the corpus into a Bag-of-words corpus\n",
        "        corpus_bow = [dictionary.doc2bow(words) for words in corpus]\n",
        "        # Construct the TF-IDF model\n",
        "        tfidf = TfidfModel(corpus_bow)\n",
        "        # Transform the whole corpus\n",
        "        corpus_tfidf = tfidf[corpus_bow]\n",
        "        # Construct the LSI model\n",
        "        model = LsiModel(corpus_tfidf, id2word=dictionary, num_topics=800)\n",
        "        return model\n",
        "\n",
        "# Construct language model\n",
        "language_model = Construct_language_model(corpus, LANGUAGE_MODEL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6M3Gdy_Gd-M"
      },
      "source": [
        "# Step 4: Train classifier models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ59mGpSKiqe"
      },
      "outputs": [],
      "source": [
        "def Convert_to_vectors(words, language_model):\n",
        "    vectors = []\n",
        "    for word_list in words:\n",
        "        if isinstance(language_model, Doc2Vec):\n",
        "            vector = language_model.infer_vector(word_list)\n",
        "        else:\n",
        "            bow = language_model.id2word.doc2bow(word_list)\n",
        "            vector = [value for _, value in language_model[bow]]\n",
        "        vectors.append(vector)\n",
        "    return vectors\n",
        "\n",
        "# Convert malware strings to vectors\n",
        "malware_vectors = Convert_to_vectors(reduced_malware_words, language_model)\n",
        "\n",
        "# Convert benign strings to vectors\n",
        "benign_vectors = Convert_to_vectors(reduced_benign_words, language_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j96k98fIj-u",
        "outputId": "7b694d08-afd7-4e50-8250-e3844eca31e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before dropna: (855, 754)\n",
            "Shape after dropna: (841, 754)\n"
          ]
        }
      ],
      "source": [
        "# Labels for malware and benign samples\n",
        "malware_labels = [1] * len(malware_vectors)  # 1 for malware\n",
        "benign_labels = [0] * len(benign_vectors)  # 0 for benign\n",
        "\n",
        "df = pd.DataFrame(malware_vectors + benign_vectors)\n",
        "\n",
        "df['label'] = malware_labels + benign_labels\n",
        "print(\"Shape before dropna:\", df.shape)\n",
        "\n",
        "df.dropna(inplace = True)\n",
        "print(\"Shape after dropna:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4rBYbyUKFg3"
      },
      "outputs": [],
      "source": [
        "X = df.drop('label', axis = 1)\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "y_train = np.array(y_train, dtype=np.int32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rngr2ePGQYok"
      },
      "outputs": [],
      "source": [
        "def Evaluate(y_pred, y_test):\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    ppv = precision_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    tpr = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f' Accuracy score: {acc}')\n",
        "    print(f'Precision score: {ppv}')\n",
        "    print(f'   Recall score: {tpr}')\n",
        "    print(f'       F1 score: {f1}')\n",
        "    print(f'Confusion matrix: \\n{confusion_matrix(y_pred, y_test)}')\n",
        "    print(classification_report(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz0S0XfPzHWm"
      },
      "source": [
        "## Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOIdZcOx6KJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e994db56-6710-46cc-f737-0b30ebd61a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy score: 0.9467455621301775\n",
            "Precision score: 0.947869876473891\n",
            "   Recall score: 0.9467455621301775\n",
            "       F1 score: 0.9448184352441353\n",
            "Confusion matrix: \n",
            "[[129   8]\n",
            " [  1  31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.97       137\n",
            "           1       0.79      0.97      0.87        32\n",
            "\n",
            "    accuracy                           0.95       169\n",
            "   macro avg       0.89      0.96      0.92       169\n",
            "weighted avg       0.95      0.95      0.95       169\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import *\n",
        "\n",
        "if LANGUAGE_MODEL == \"Doc2Vec\":\n",
        "    svm_model = SVC()\n",
        "else:\n",
        "    svm_model = SVC()\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "Evaluate(svm_model.predict(X_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jgm_QnOc6G"
      },
      "source": [
        "## Random Forests (RF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuuzGk0t6SDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7376cc07-3f6a-4542-8c97-a68f74b6d82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy score: 0.9649122807017544\n",
            "Precision score: 0.9665071770334929\n",
            "   Recall score: 0.9649122807017544\n",
            "       F1 score: 0.9640671446057003\n",
            "Confusion matrix: \n",
            "[[126   6]\n",
            " [  0  39]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98       132\n",
            "           1       0.87      1.00      0.93        39\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.93      0.98      0.95       171\n",
            "weighted avg       0.97      0.96      0.97       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "if LANGUAGE_MODEL == \"Doc2Vec\":\n",
        "    rf_model = RandomForestClassifier()\n",
        "else:\n",
        "    rf_model = RandomForestClassifier()\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "Evaluate(rf_model.predict(X_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2BscelUOlKi"
      },
      "source": [
        "## XGBoost (XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJc3ad196VfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838687f1-850e-477e-df7a-0fcce3238ca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy score: 0.9590643274853801\n",
            "Precision score: 0.9612188365650969\n",
            "   Recall score: 0.9590643274853801\n",
            "       F1 score: 0.9578913091912458\n",
            "Confusion matrix: \n",
            "[[126   7]\n",
            " [  0  38]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97       133\n",
            "           1       0.84      1.00      0.92        38\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.92      0.97      0.94       171\n",
            "weighted avg       0.97      0.96      0.96       171\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "if LANGUAGE_MODEL == \"Doc2Vec\":\n",
        "    xgb_model = XGBClassifier() #, min_child_weight=11)\n",
        "else:\n",
        "    xgb_model = XGBClassifier()\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "Evaluate(xgb_model.predict(X_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts0Yce2YOp8U"
      },
      "source": [
        "## Multi-Layer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdrI2ezo6ZDL"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "import chainer as ch\n",
        "from chainer import datasets\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jItVqfz6yNX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame as D\n",
        "from pandas import read_csv as R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNVK6OzX667P",
        "outputId": "e8f35e69-aeed-4bfb-efca-92a3a5c49b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
            "\u001b[J1           0.30002     0.0924698             0.929927       0.977143                  0.653747      \n",
            "\u001b[J2           0.0388703   0.034618              0.992701       0.988571                  1.5838        \n",
            "\u001b[J3           0.0183548   0.03415               0.99562        0.988571                  2.60228       \n",
            "\u001b[J4           0.0121514   0.0271868             0.99562        0.988571                  3.93425       \n",
            "\u001b[J5           0.00891736  0.0352936             0.998529       0.988571                  5.04953       \n",
            "\u001b[J6           0.00729814  0.0333827             0.99854        0.988571                  6.56494       \n",
            "\u001b[J7           0.00625781  0.034187              0.99854        0.988571                  7.90589       \n"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "train_mlp = datasets.TupleDataset(X_train, y_train[:, None])\n",
        "test_mlp = datasets.TupleDataset(X_test, y_test[:, None])\n",
        "\n",
        "# Iterator\n",
        "train_iter = ch.iterators.SerialIterator(train_mlp, 5)\n",
        "test_iter = ch.iterators.SerialIterator(test_mlp, 5, repeat=False, shuffle=False)\n",
        "\n",
        "# Model\n",
        "def MLP(n_units, n_out):\n",
        "    layer = ch.Sequential(L.Linear(n_units), F.relu)\n",
        "    model = layer.repeat(2)\n",
        "    model.append(L.Linear(n_out))\n",
        "    return model\n",
        "\n",
        "mlp_model = L.Classifier(MLP(40, 1), lossfun=F.sigmoid_cross_entropy, accfun=F.binary_accuracy)\n",
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam().setup(mlp_model)\n",
        "\n",
        "# Create the updater, using the optimizer\n",
        "updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n",
        "\n",
        "# Set up a trainer\n",
        "trainer = training.Trainer(updater, (7, 'epoch'), out='mlp_result')\n",
        "# Evaluate the model with the test dataset for each epoch\n",
        "trainer.extend(extensions.Evaluator(test_iter, mlp_model, device=-1))\n",
        "\n",
        "# Dump a computational graph from 'loss' variable at the first iteration\n",
        "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
        "trainer.extend(extensions.DumpGraph('main/loss'))\n",
        "\n",
        "trainer.extend(extensions.snapshot(), trigger=(20, 'epoch'))\n",
        "\n",
        "# Write a log of evaluation statistics for each epoch\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "# Save two plot images to the result dir\n",
        "trainer.extend(\n",
        "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
        "                          'epoch', file_name='loss.png'))\n",
        "trainer.extend(\n",
        "    extensions.PlotReport(\n",
        "        ['main/accuracy', 'validation/main/accuracy'],\n",
        "        'epoch', file_name='accuracy.png'))\n",
        "\n",
        "# Print selected entries of the log to stdout\n",
        "trainer.extend(extensions.PrintReport(\n",
        "    ['epoch', 'main/loss', 'validation/main/loss',\n",
        "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "# main loop\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = mlp_model.predictor(test_mlp._datasets[0]).array\n",
        "y_pred = np.where(y_pred < 0, 0, 1)\n",
        "\n",
        "Evaluate(y_pred, y_test)"
      ],
      "metadata": {
        "id": "fu5UQbtQIGV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9197a33-caa3-4c32-b0bf-98a6e9903955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy score: 0.9883040935672515\n",
            "Precision score: 0.9884868421052632\n",
            "   Recall score: 0.9883040935672515\n",
            "       F1 score: 0.988217232415326\n",
            "Confusion matrix: \n",
            "[[126   2]\n",
            " [  0  43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       128\n",
            "           1       0.96      1.00      0.98        43\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.98      0.99      0.98       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpXB8n6_6ANI"
      },
      "source": [
        "## Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_fts = X_train.shape[1]"
      ],
      "metadata": {
        "id": "sRO9RHTmKELk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V35tiaqoOyBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d34cc10-e69b-43b4-cd10-8926ebd0413d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
            "\u001b[J1           0.322993    0.0407846             0.90219        0.988571                  17.427        \n",
            "\u001b[J2           0.0420772   0.100828              0.988321       0.971429                  33.4187       \n",
            "\u001b[J3           0.0372309   0.0375048             0.988321       0.988571                  61.218        \n"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "train_cnn = datasets.TupleDataset(X_train.reshape(-1, 1, num_fts), y_train[:, None])\n",
        "test_cnn = datasets.TupleDataset(X_test.reshape(-1, 1, num_fts), y_test[:, None])\n",
        "\n",
        "# Iterator\n",
        "train_iter = ch.iterators.SerialIterator(train_cnn, 5)\n",
        "test_iter = ch.iterators.SerialIterator(test_cnn, 5, repeat=False, shuffle=False)\n",
        "\n",
        "# Model\n",
        "class CNN(ch.Chain):\n",
        "    def __init__(self, n_out):\n",
        "        super(CNN, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution1D(None, 32, ksize=3, stride=1, pad=1)\n",
        "            self.conv2 = L.Convolution1D(32, 64, ksize=3, stride=1, pad=1)\n",
        "            self.fc1 = L.Linear(None, 512)\n",
        "            self.fc2 = L.Linear(512, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.conv1(x))\n",
        "        h = F.max_pooling_1d(h, 2, 2)\n",
        "        h = F.relu(self.conv2(h))\n",
        "        h = F.max_pooling_1d(h, 2, 2)\n",
        "        h = F.relu(self.fc1(h))\n",
        "        h = self.fc2(h)\n",
        "        return h\n",
        "\n",
        "cnn_model = L.Classifier(CNN(1), lossfun=F.sigmoid_cross_entropy, accfun=F.binary_accuracy)\n",
        "\n",
        "# Setup an optimizer\n",
        "optimizer = ch.optimizers.Adam().setup(cnn_model)\n",
        "\n",
        "# Create the updater, using the optimizer\n",
        "updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n",
        "\n",
        "# Set up a trainer\n",
        "trainer = training.Trainer(updater, (3, 'epoch'), out='cnn_result')\n",
        "\n",
        "# Evaluate the model with the test dataset for each epoch\n",
        "trainer.extend(extensions.Evaluator(test_iter, cnn_model, device=-1))\n",
        "\n",
        "# Dump a computational graph from 'loss' variable at the first iteration\n",
        "trainer.extend(extensions.DumpGraph('main/loss'))\n",
        "\n",
        "# Write a log of evaluation statistics for each epoch\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "# Save two plot images to the result dir\n",
        "trainer.extend(\n",
        "    extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
        "                          'epoch', file_name='loss.png'))\n",
        "trainer.extend(\n",
        "    extensions.PlotReport(\n",
        "        ['main/accuracy', 'validation/main/accuracy'],\n",
        "        'epoch', file_name='accuracy.png'))\n",
        "\n",
        "# Print selected entries of the log to stdout\n",
        "trainer.extend(extensions.PrintReport(\n",
        "    ['epoch', 'main/loss', 'validation/main/loss',\n",
        "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "\n",
        "# Main loop\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction and evaluation\n",
        "y_pred = cnn_model.predictor(test_cnn._datasets[0]).array\n",
        "y_pred = np.where(y_pred < 0, 0, 1)\n",
        "\n",
        "Evaluate(y_pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yNCN1Q3R9ZT",
        "outputId": "aacecc5f-1961-481e-bed2-1f9a56de7c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy score: 0.9883040935672515\n",
            "Precision score: 0.9884868421052632\n",
            "   Recall score: 0.9883040935672515\n",
            "       F1 score: 0.988217232415326\n",
            "Confusion matrix: \n",
            "[[126   2]\n",
            " [  0  43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       128\n",
            "           1       0.96      1.00      0.98        43\n",
            "\n",
            "    accuracy                           0.99       171\n",
            "   macro avg       0.98      0.99      0.98       171\n",
            "weighted avg       0.99      0.99      0.99       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvlWLfH4RYTR"
      },
      "source": [
        "# Step 5: Detect unknown malware"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_random(input_list, ratio):\n",
        "    # Shuffle the list to randomize the order\n",
        "    random.shuffle(input_list)\n",
        "    if ratio < 1:\n",
        "        length = int(len(input_list) * ratio)\n",
        "    else:\n",
        "        length = int(ratio)\n",
        "    return input_list[:length]"
      ],
      "metadata": {
        "id": "oNdSsYz7Osfb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unknown_files = get_random(benign_files, 10)\n",
        "unknown_files += get_random(malicious_files, 10)\n"
      ],
      "metadata": {
        "id": "Occy46bGQZJl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cOsNCC4Ng7qZ"
      },
      "outputs": [],
      "source": [
        "# Extract printable strings\n",
        "unknown_words = []\n",
        "\n",
        "# Extract printable strings from unknown executable files\n",
        "for file_path in unknown_files:\n",
        "    strings = Strings(file_path)\n",
        "    words = String2word(strings)\n",
        "    unknown_words.append(words)\n",
        "\n",
        "del strings, words, file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wdUGAYgyRsiZ"
      },
      "outputs": [],
      "source": [
        "# Reduce the words\n",
        "reduced_unknown_words = Reduce_words(unknown_words, benign_frequent_words+malware_frequent_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xRFc-ul2VI0d"
      },
      "outputs": [],
      "source": [
        "# Convert unknown words to vectors\n",
        "unknown_vectors = Convert_to_vectors(reduced_unknown_words, language_model)\n",
        "unknown_vectors = np.array(unknown_vectors, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnZXqvkXU5gP",
        "outputId": "ed60bd25-6bd8-4e9b-b03a-6c748e172634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          SVM predictions: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 1]\n",
            "Random Forest predictions: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
            "      XGBoost predictions: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
            "          MLP predictions: [0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
            "          CNN predictions: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Classify the feature vectors\n",
        "\n",
        "# Predictions from different models\n",
        "svm_predictions = svm_model.predict(unknown_vectors)\n",
        "rf_predictions = rf_model.predict(unknown_vectors)\n",
        "xgb_predictions = xgb_model.predict(unknown_vectors)\n",
        "mlp_predictions = np.where(mlp_model.predictor(np.array(unknown_vectors)).array < 0, 0, 1)\n",
        "cnn_predictions = np.where(cnn_model.predictor(np.array(unknown_vectors).reshape(-1, 1, unknown_vectors.shape[1])).array < 0, 0, 1)\n",
        "\n",
        "# Convert all predictions to flat numpy arrays\n",
        "svm_predictions_flat = np.array(svm_predictions).flatten()\n",
        "rf_predictions_flat = np.array(rf_predictions).flatten()\n",
        "xgb_predictions_flat = np.array(xgb_predictions).flatten()\n",
        "mlp_predictions_flat = np.array(mlp_predictions).flatten()\n",
        "cnn_predictions_flat = np.array(cnn_predictions).flatten()\n",
        "\n",
        "# Combine all predictions into a single array for consistency\n",
        "all_predictions = {\n",
        "    '          SVM': svm_predictions_flat,\n",
        "    'Random Forest': rf_predictions_flat,\n",
        "    '      XGBoost': xgb_predictions_flat,\n",
        "    '          MLP': mlp_predictions_flat,\n",
        "    '          CNN': cnn_predictions_flat\n",
        "}\n",
        "\n",
        "# Print the predictions\n",
        "for model, predictions in all_predictions.items():\n",
        "    print(f\"{model} predictions: {predictions}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Save model and material"
      ],
      "metadata": {
        "id": "YPExBHlHEqvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/NLP_material/'"
      ],
      "metadata": {
        "id": "x5ooczxVXzmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def save_language_model(model, model_type):\n",
        "    if model_type == \"Doc2Vec\":\n",
        "        model.save(save_path+model_type)\n",
        "    else:\n",
        "        model.save(save_path+model_type + '.lsi')  # Lưu LsiModel\n",
        "\n",
        "# Assuming corpus_tfidf is available in the current scope\n",
        "save_language_model(language_model, LANGUAGE_MODEL)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PqrY6Z8OVLFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Lưu danh sách xuống file\n",
        "with open(save_path+LANGUAGE_MODEL+'_benign_frequent_words.json', 'w') as file:\n",
        "    json.dump(benign_frequent_words, file)\n",
        "\n",
        "with open(save_path+LANGUAGE_MODEL+'_malware_frequent_words.json', 'w') as file:\n",
        "    json.dump(malware_frequent_words, file)"
      ],
      "metadata": {
        "id": "v-PLUAYpH5O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(svm_model, save_path+f\"_{LANGUAGE_MODEL}-SVM.pkl\")\n",
        "joblib.dump(rf_model, save_path+f\"_{LANGUAGE_MODEL}-RF.pkl\")\n",
        "joblib.dump(xgb_model, save_path+f\"_{LANGUAGE_MODEL}-XGB.pkl\")\n",
        "ch.serializers.save_npz(save_path+f\"_{LANGUAGE_MODEL}_mlp_model.npz\", mlp_model)\n",
        "ch.serializers.save_npz(save_path+f\"_{LANGUAGE_MODEL}_cnn_model.npz\", cnn_model)\n"
      ],
      "metadata": {
        "id": "KgEUQQL5Excz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Reload Testing"
      ],
      "metadata": {
        "id": "CLG49R5zP3kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "import chainer as ch\n",
        "import chainer.links as L\n",
        "import chainer.functions as F\n",
        "\n",
        "from gensim.models import Doc2Vec, LsiModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "MAX_FILESIZE = 16*1024*1024\n",
        "MAX_STRINGCNT = 2048\n",
        "MAX_STRINGLEN = 1024\n",
        "LANGUAGE_MODEL = 'LSI'\n",
        "save_path = '/content/drive/MyDrive/NLP_material/'\n",
        "\n",
        "def Strings(file_path):\n",
        "    data = open(file_path, \"rb\").read(MAX_FILESIZE)\n",
        "    strings = []\n",
        "    for s in re.findall(b\"[\\x1f-\\x7e]{6,}\", data):\n",
        "        strings.append(s.decode(\"utf-8\"))\n",
        "    for s in re.findall(b\"(?:[\\x1f-\\x7e][\\x00]){6,}\", data):\n",
        "        strings.append(s.decode(\"utf-16le\"))\n",
        "\n",
        "    # Now limit the amount & length of the strings.\n",
        "    strings = strings[:MAX_STRINGCNT]\n",
        "    for idx, s in enumerate(strings):\n",
        "        strings[idx] = s[:MAX_STRINGLEN]\n",
        "\n",
        "    return strings\n",
        "\n",
        "def String2word(string_list):\n",
        "    # Join the list of strings into a single string\n",
        "    full_string = ' '.join(string_list)\n",
        "    # Split the full string into words\n",
        "    return full_string.lower().split()\n",
        "\n",
        "def Reduce_words(words, frequent_words):\n",
        "    reduced_words = []\n",
        "    for file_words in words:\n",
        "        reduced_file_words = [word for word in file_words if word in frequent_words]\n",
        "        reduced_words.append(reduced_file_words)\n",
        "    return reduced_words\n",
        "\n",
        "def Convert_to_vectors(words, language_model):\n",
        "    vectors = []\n",
        "    for word_list in words:\n",
        "        if isinstance(language_model, Doc2Vec):\n",
        "            vector = language_model.infer_vector(word_list)\n",
        "        else:\n",
        "            bow = language_model.id2word.doc2bow(word_list)\n",
        "            vector = [value for _, value in language_model[bow]]\n",
        "        vectors.append(vector)\n",
        "    return vectors\n",
        "\n",
        "def load_language_model(filename):\n",
        "    if filename.endswith(\"Doc2Vec\"):\n",
        "        return Doc2Vec.load(filename)\n",
        "    else:\n",
        "        return LsiModel.load(filename + '.lsi')\n",
        "\n",
        "def MLP(n_units, n_out):\n",
        "    layer = ch.Sequential(L.Linear(n_units), F.relu)\n",
        "    model = layer.repeat(2)\n",
        "    model.append(L.Linear(n_out))\n",
        "    return model\n",
        "\n",
        "class CNN(ch.Chain):\n",
        "    def __init__(self, n_out):\n",
        "        super(CNN, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution1D(None, 32, ksize=3, stride=1, pad=1)\n",
        "            self.conv2 = L.Convolution1D(32, 64, ksize=3, stride=1, pad=1)\n",
        "            self.fc1 = L.Linear(None, 512)\n",
        "            self.fc2 = L.Linear(512, n_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.conv1(x))\n",
        "        h = F.max_pooling_1d(h, 2, 2)\n",
        "        h = F.relu(self.conv2(h))\n",
        "        h = F.max_pooling_1d(h, 2, 2)\n",
        "        h = F.relu(self.fc1(h))\n",
        "        h = self.fc2(h)\n",
        "        return h\n",
        "\n",
        "# Extract printable strings\n",
        "unknown_words = []\n",
        "strings = Strings(benign_files[10])\n",
        "words = String2word(strings)\n",
        "unknown_words.append(words)\n",
        "\n",
        "del strings, words\n",
        "\n",
        "with open(save_path+LANGUAGE_MODEL+'_benign_frequent_words.json', 'r') as file:\n",
        "    benign_frequent_words = json.load(file)\n",
        "\n",
        "with open(save_path+LANGUAGE_MODEL+'_malware_frequent_words.json', 'r') as file:\n",
        "    malware_frequent_words = json.load(file)\n",
        "# Reduce the words\n",
        "reduced_unknown_words = Reduce_words(unknown_words, benign_frequent_words+malware_frequent_words)\n",
        "\n",
        "language_model = load_language_model(save_path+LANGUAGE_MODEL)\n",
        "\n",
        "unknown_vectors = Convert_to_vectors(reduced_unknown_words, language_model)\n",
        "unknown_vectors = np.array(unknown_vectors, dtype=np.float32)\n",
        "\n",
        "# Load các mô hình đã lưu bằng joblib\n",
        "svm_model = joblib.load(save_path+f\"_{LANGUAGE_MODEL}-SVM.pkl\")\n",
        "rf_model = joblib.load(save_path+f\"_{LANGUAGE_MODEL}-RF.pkl\")\n",
        "xgb_model = joblib.load(save_path+f\"_{LANGUAGE_MODEL}-XGB.pkl\")\n",
        "\n",
        "# Load các mô hình đã lưu bằng chainer\n",
        "mlp_model = L.Classifier(MLP(40, 1), lossfun=F.sigmoid_cross_entropy, accfun=F.binary_accuracy)\n",
        "ch.serializers.load_npz(save_path+f\"_{LANGUAGE_MODEL}_mlp_model.npz\", mlp_model)\n",
        "\n",
        "cnn_model = L.Classifier(CNN(1), lossfun=F.sigmoid_cross_entropy, accfun=F.binary_accuracy)\n",
        "ch.serializers.load_npz(save_path+f\"_{LANGUAGE_MODEL}_cnn_model.npz\", cnn_model)\n",
        "\n",
        "# Classify the feature vectors\n",
        "\n",
        "# Predictions from different models\n",
        "svm_predictions = svm_model.predict(unknown_vectors)\n",
        "rf_predictions = rf_model.predict(unknown_vectors)\n",
        "xgb_predictions = xgb_model.predict(unknown_vectors)\n",
        "mlp_predictions = np.where(mlp_model.predictor(np.array(unknown_vectors)).array < 0, 0, 1)\n",
        "cnn_predictions = np.where(cnn_model.predictor(np.array(unknown_vectors).reshape(-1, 1, unknown_vectors.shape[1])).array < 0, 0, 1)\n",
        "\n",
        "# Convert all predictions to flat numpy arrays\n",
        "svm_predictions_flat = np.array(svm_predictions).flatten()\n",
        "rf_predictions_flat = np.array(rf_predictions).flatten()\n",
        "xgb_predictions_flat = np.array(xgb_predictions).flatten()\n",
        "mlp_predictions_flat = np.array(mlp_predictions).flatten()\n",
        "cnn_predictions_flat = np.array(cnn_predictions).flatten()\n",
        "\n",
        "# Combine all predictions into a single array for consistency\n",
        "all_predictions = {\n",
        "    '          SVM': svm_predictions_flat,\n",
        "    'Random Forest': rf_predictions_flat,\n",
        "    '      XGBoost': xgb_predictions_flat,\n",
        "    '          MLP': mlp_predictions_flat,\n",
        "    '          CNN': cnn_predictions_flat\n",
        "}\n",
        "\n",
        "# Print the predictions\n",
        "for model, predictions in all_predictions.items():\n",
        "    # f\"{model} predictions: {predictions}\"\n",
        "    print(f\"{model} predictions: {predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xFbcj8MP1sa",
        "outputId": "af749ef9-6e27-4587-f538-0265d5da3207"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          SVM predictions: [0]\n",
            "Random Forest predictions: [0]\n",
            "      XGBoost predictions: [0]\n",
            "          MLP predictions: [0]\n",
            "          CNN predictions: [0]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}